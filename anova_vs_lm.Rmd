---
title: "Anova vs Linear regression"
output:
  html_document:
    df_print: paged
    
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---


One way anova and linear regression are almost identical. There is no difference in the results because reference level is considered as intercept in the model. Other level effects are just the differential effect they have from the intercept. Let us give an example below.

```{r, warning=FALSE}
library(MASS)
data("crabs")
attach(crabs)
dat<- cbind(crabs["sp"], crabs["BD"]) #What we are doing here is creating a 
                                      #simple dataset for one-way ANOVA

lm(BD~sp)
```
Now, let's see what output we get from "aov" command in R. 

```{r}
aov(BD~sp)$coef
```
It looks like we are getting the same output from both the commands. To really look at what is happening here, let us construct out models a little bit differently. 

```{r}
species_1 <- rep(-1, length(sp))
species_2 <- ifelse(sp == "O", 1, 0)
lm(BD ~ species_2 + species_1)

```
What is happening here is that, R is using treatment contrast by default which is shown here through the use of contrast coding. We have coded the reference level with -1 and other level as 1 when true and 0 otherwise. Both "aov" and "lm" in R does this coding in background to considers the mean outcome at the reference level as intercept. The other one then just becomes the difference between effects. To demonstrate that a little bit further, let us see the mean outcome by levels.

```{r}
tapply(BD, factor(sp) , mean)
```
We can compare the last two results and see that the mean effect of level "B" is the intercept in the model. The coefficient of "O" (Species 2) is  the difference between mean effects (15.478-12.583 = 2.895). 

## Ancova 

Let us now include one of the continuous variables in the model. Here, we add "FL" a our co-variate. 

```{r}
lm(BD ~ species_2 + species_1 + FL)
```

We can see from the results that every estimates has changed and there is a new estimate for the included co-variate. Now, let us discuss how did that happen. Since, there are both categorical and continuous variables in the model, we need to first find the outcome adjusted for co-variate. We are going to use this adjusted values, i.e. residuals of first regression, in the second regression to estimate the coefficients of the levels of categorical variables. Now, to avoid a further problem, let estimate the first regression without any intercept. We do that by de-meaning the co-variate.

```{r}
x_cv <- FL - mean(FL)
mod1 <- lm(BD~-1+x_cv)
lm(mod1$residual ~ species_2 + species_1 + x_cv)
```
We see that the coefficient for species_2 is the same as before. To match the coefficient of the co-variate as well, we need to simply add the estimates of the two models. 

```{r}
mod1$coef + 0.004689
```
Finally, we can see a large difference between the two estimates of the intercepts. We need to carefully look at the fact that we are using a demeaned co-variate. So, the difference of the estimated intercept and the global mean of co-variate times the estimated coefficient should be equal to the result.   

```{r}
14.067825- mean(FL)*0.972381
```
Let us explain the mathematical background behind these operations. 

$$y_{res} = y - b(X-\bar{X})=\mu + \alpha + \beta(X-\bar{X})$$
$$=> y = \mu + \alpha + (b + \beta)(X-\bar{X})$$
$$=> y = (\mu - (b + \beta)\bar{X}) + \alpha + (b + \beta)X$$

## Factorial Design

We are going to introduce a second categorical variable "sex" into the dataframe. Since, we have discussed about estimating main effects from one-way anova already, we are going to focus more on estimating the interaction effect of two-way anova.

```{r}
dat1 <-  cbind(crabs["sp"], crabs["sex"], crabs["BD"])
lm(BD~sp*sex)
```
We understand from the result that R has considered "spoB" and "sexF" as reference. Which is evident from the following linear model as well.

```{r}
spB<- ifelse(sp == "B", 1, 0)
spO<- ifelse(sp == "O", 1, 0)
sexM<- ifelse(sex == "M", 1, 0)
sexF<- ifelse(sex == "F", 1, 0)
lm(BD ~ spO + spB + sexM + sexF + spO * sexM + spB * sexM + spO * sexF + spB * sexF)
```
Now, let us interpret the model parameters. What is happening here is more apparent when we look at the following table.

```{r, warnings = F, message= F}
library(dplyr)
dat1 %>% group_by(sp, sex) %>% summarize(mean(BD))
```
Since, both "spB" and "sexF" are reference, the mean outcome of that category is our intercept this time. The main effects are calculated like the models before by subtracting from this reference group. Once we have the main effect, we can now estimate the interaction effect by any of the equations below.

$$E[BD|sp = O, sex = M] - E[BD|sp = B, sex = F] = \alpha + \gamma + \delta$$
$$E[BD|sp = O, sex = M] - E[BD|sp = O, sex = F] = \gamma + \delta$$
$$E[BD|sp = O, sex = M] - E[BD|sp = B, sex = M] = \alpha + \delta$$

where, $\alpha$, $\gamma$ and $\delta$ are main effect of "sp", main effect of "sex" and interaction effect of "sp" and "sex" respectively. 

